{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import urllib.parse\n",
    "from typing import List, Dict\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "from schemas.chat_schema import ChatResponse, Language, ChatRequest\n",
    "from schemas.location_schema import LocationQuery\n",
    "from services.location_service import LocationService\n",
    "\n",
    "\n",
    "class ChatService():\n",
    "\n",
    "    def __init__(self, lang=Language.ENGLISH, location=\"Garching, Munich\"):\n",
    "        self.chat_history_store = [] # TODO: Temporary storage for chat history (should be replaced by a database in production)\n",
    "        self.llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "        self.lang = lang\n",
    "        self.location = location\n",
    "        self.video_store = []\n",
    "        self.location_service = LocationService()\n",
    "        self._setup_location_extractor()\n",
    "        self._setup_video_store()\n",
    "        self._setup_retriever()\n",
    "        self._setup_rag_chain(lang)\n",
    "    \n",
    "    def setup(self, request):\n",
    "        self.__init__(lang=request.language)\n",
    "\n",
    "    def _setup_video_store(self):\n",
    "        with open('utils/db/videos.json') as file:\n",
    "            self.video_store = json.load(file)\n",
    "\n",
    "    def _setup_retriever(self):\n",
    "        \"\"\"\n",
    "        Loads the reference documents, combines video json objects, creates a vector store, and a retriever.\n",
    "        \"\"\"\n",
    "        # Load, split, and index documents\n",
    "        loader = PyPDFDirectoryLoader(\"utils/db/\")\n",
    "        docs = loader.load()\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "        splits = text_splitter.split_documents(docs)\n",
    "\n",
    "        # Add video metadata as documents\n",
    "        video_documents = [\n",
    "            Document(\n",
    "                page_content=f\"{video['title']} {video['description']} {' '.join(video['tags'])}\",\n",
    "                metadata={\n",
    "                    \"title\": video[\"title\"],\n",
    "                    \"description\": video[\"description\"],\n",
    "                    \"tags\": video[\"tags\"],\n",
    "                    \"source\": video[\"provider\"],\n",
    "                    \"url\": video[\"url\"],\n",
    "                    \"category\": video[\"category\"]\n",
    "                }\n",
    "            )\n",
    "            for video in self.video_store\n",
    "        ]\n",
    "\n",
    "        all_documents = splits + video_documents\n",
    "\n",
    "        vectorstore = InMemoryVectorStore.from_documents(documents=all_documents, embedding=OpenAIEmbeddings(model=\"text-embedding-ada-002\"))\n",
    "        retriever = vectorstore.as_retriever(search_type=\"mmr\", search_kwargs={'k': 5, 'fetch_k': 100, \"score_threshold\": 0.75})\n",
    "\n",
    "        # Contextualize question \n",
    "        contextualize_q_system_prompt = (\n",
    "            \"\"\"\n",
    "            Given the user's question and related context, rewrite the question in a way that maximizes its clarity and semantic meaning. \n",
    "            Ensure no ambiguity remains.\n",
    "            \"\"\"\n",
    "        )\n",
    "        contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                (\"system\", contextualize_q_system_prompt),\n",
    "                MessagesPlaceholder(\"chat_history\"),\n",
    "                (\"human\", \"{input}\"),\n",
    "            ]\n",
    "        )\n",
    "        self.history_aware_retriever = create_history_aware_retriever(\n",
    "            self.llm, retriever, contextualize_q_prompt\n",
    "            )\n",
    "        \n",
    "    def _setup_rag_chain(self, lang):\n",
    "        with open(f'utils/prompts/prompt_klaro_{lang.value}.txt', 'r') as file:\n",
    "            system_prompt = file.read()\n",
    "        \n",
    "        system_prompt = system_prompt.replace(\"{location}\", self.location)\n",
    "        \n",
    "        qa_prompt = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                (\"system\", system_prompt),\n",
    "                MessagesPlaceholder(\"chat_history\"),\n",
    "                (\"human\", \"{input}\"),\n",
    "            ]\n",
    "        )\n",
    "        question_answer_chain = create_stuff_documents_chain(self.llm, qa_prompt)\n",
    "        self.rag_chain = create_retrieval_chain(self.history_aware_retriever, question_answer_chain)\n",
    "    \n",
    "    def _extract_cleaned_sources(self, context) -> List[str]:\n",
    "        sources = []\n",
    "        for item in context:\n",
    "            if 'source' in item.metadata:\n",
    "                # Get the source name and remove \"utils/db/\" and \".pdf\"\n",
    "                source = item.metadata['source'].replace(\"utils/db/\", \"\").replace(\".pdf\", \"\")\n",
    "                sources.append(source)\n",
    "        return list(set(sources))\n",
    "\n",
    "    def _fill_thumbnails(self, video_sources) -> List[str]:\n",
    "        \"\"\"\n",
    "        Converts video URLs into thumbnail URL provided by YouTube.\n",
    "        \"\"\"\n",
    "        thumbnails = []\n",
    "        for video in video_sources:\n",
    "            parsed_url = urllib.parse.urlparse(video['url'])\n",
    "            query_params = urllib.parse.parse_qs(parsed_url.query)\n",
    "            # Extract the video ID\n",
    "            video_id = query_params.get(\"v\", [None])[0]\n",
    "            if  video_id:\n",
    "                thumbnail_url = f\"https://img.youtube.com/vi/{video_id}/0.jpg\"\n",
    "                thumbnails.append(thumbnail_url)\n",
    "        \n",
    "        return thumbnails\n",
    "    \n",
    "    def _setup_location_extractor(self):\n",
    "        \"\"\"\n",
    "        Creates a chain that expects AI generated message as input, and returns a list of a predifined location types.\n",
    "        The chain tries to infer if a location type is mentioned in the AI generated message.\n",
    "        \"\"\"\n",
    "        with open(f'utils/prompts/prompt_location_extractor.txt', 'r') as file:\n",
    "            location_prompt = file.read()\n",
    "        \n",
    "        prompt = ChatPromptTemplate.from_messages([(\"system\", location_prompt), (\"human\", \"{input}\")])\n",
    "        location_extractor = self.llm.with_structured_output(LocationQuery)\n",
    "        self.location_extractor_llm = prompt | location_extractor\n",
    "\n",
    "\n",
    "    async def _check_locations(self, answer) -> Dict:\n",
    "        \"\"\"\n",
    "        Asks location extractor chain with AI generated message if a location type is mentioned in the message.\n",
    "        If it is, then it adds the user's location, and sends a query to Google Maps API, to extract the places\n",
    "        that match the location types and the location.\n",
    "        \"\"\"\n",
    "        search_text: LocationQuery = self.location_extractor_llm.invoke(answer)\n",
    "\n",
    "        if search_text.query:\n",
    "            print(search_text.concat() + f\" near {self.location}\")\n",
    "            return await self.location_service.get_places(search_text.concat() + f\" near {self.location}\")\n",
    "        else:\n",
    "            return {}\n",
    "        \n",
    "    async def _extract_context(self, response):\n",
    "        \"\"\"\n",
    "        Extract video and location data from the RAG chain response.\n",
    "        Combine this data into a unified context for generating better responses.\n",
    "        \"\"\"\n",
    "        # Extract locations\n",
    "        locations = await self._check_locations(response[\"answer\"])\n",
    "\n",
    "        # Extract video metadata\n",
    "        video_sources = [\n",
    "            {\n",
    "                \"title\": item.metadata.get(\"title\"),\n",
    "                \"description\": item.metadata.get(\"description\"),\n",
    "                \"url\": item.metadata.get(\"url\"),\n",
    "            }\n",
    "            for item in response[\"context\"]\n",
    "            if item.metadata.get(\"url\")  # Check if the source is a video\n",
    "        ]\n",
    "\n",
    "        # Combine extracted context\n",
    "        unified_context = {\n",
    "            \"locations\": locations,\n",
    "            \"videos\": video_sources,\n",
    "            \"original_context\": response[\"context\"],\n",
    "        }\n",
    "\n",
    "        return unified_context\n",
    "\n",
    "\n",
    "    async def query(self, request) -> ChatResponse:\n",
    "        \"\"\"\n",
    "        The flow of this method:\n",
    "        1 - Generate an answer for the user's question using the RAG chain.\n",
    "        2 - Extract locations and videos.\n",
    "        3 - Use unified context for final output generation.\n",
    "        \"\"\"\n",
    "        self.chat_history_store.append({\"role\": \"user\", \"content\": request.input})\n",
    "\n",
    "        # Step 1: Generate initial answer with RAG chain\n",
    "        state = {\n",
    "            \"input\": request.input,\n",
    "            \"chat_history\": self.chat_history_store,\n",
    "            \"context\": \"\",\n",
    "        }\n",
    "        response = self.rag_chain.invoke(state)\n",
    "\n",
    "        # Step 2: Extract unified context\n",
    "        unified_context = await self._extract_context(response)\n",
    "\n",
    "        print(\"Unified Context:\")\n",
    "        print(unified_context)\n",
    "\n",
    "        # Step 3: Pass unified context into a secondary prompt for final refinement\n",
    "        final_prompt = (\n",
    "            \"Here is the retrieved context:\\n\"\n",
    "            f\"Locations: {unified_context['locations']}\\n\"\n",
    "            f\"Videos: {unified_context['videos']}\\n\\n\"\n",
    "            \"Based on this context, generate an accurate and user-friendly response:\\n\"\n",
    "            f\"{response['answer']}\"\n",
    "        )\n",
    "\n",
    "        final_response = self.llm(final_prompt)\n",
    "\n",
    "        # Update chat history\n",
    "        self.chat_history_store.append({\"role\": \"assistant\", \"content\": final_response})\n",
    "\n",
    "        print(\"Final Response:\")\n",
    "        print(final_response)\n",
    "\n",
    "        return ChatResponse(\n",
    "            chat_history=self.chat_history_store,\n",
    "            sources=self._extract_cleaned_sources(unified_context[\"original_context\"]),\n",
    "            thumbnails=self._fill_thumbnails(unified_context[\"videos\"]),\n",
    "            video_URLs=[video[\"url\"] for video in unified_context[\"videos\"]],\n",
    "            locations=unified_context[\"locations\"],\n",
    "            answer=final_response.content,\n",
    "        )\n",
    "\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "service = ChatService()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from schemas.chat_schema import ChatRequest\n",
    "\n",
    "request = ChatRequest(input=\"I don't know how to turn my in dad in bed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unified Context:\n",
      "{'locations': {}, 'videos': [{'title': 'How Do You Assist Someone to Walk in Care', 'description': 'How to assist someone getting off the chair, walking and sitting back.', 'url': 'https://www.youtube.com/watch?v=3KHgy7zQ4MA'}], 'original_context': [Document(id='5a0a0457-7ec5-4374-877e-5a46298fc237', metadata={'source': 'utils/db/Caregiver Manual by Help Age International.pdf', 'page': 30}, page_content='Moving in and out of bed │ Page 4/9 \\n4. if a second caregiver or family member can help, ask them to stand on the \\nother side of the bed;  \\n5. cross the persons arms;  \\n6. bend the person’s leg nearest to you so that their foot is close to their bottom, \\nor if this is not possible, cross their legs at their ankles; \\n7. place one hand on the person’s shoulder and the other on their hip. Gently \\npush the person over onto their side, \\nmaking sure in advance that they are not \\ntoo close to the edge of the bed; \\n8. if there is a second caregiver on the \\nother side of the bed, they can help \\nsupport the person by placing their \\nhands on the top shoulder and knee \\n9. if possible, place a cushion or pillow \\nbehind the person’s back and between \\ntheir legs; \\n10. support the person in this position, check \\nhow they are feeling and make sure they \\nare comfortable and feeling safe.  \\nNote: A bed sheet can be used to help turn a heavier person in bed and may be useful \\nin other circumstances.'), Document(id='6df6e107-573e-4b2f-a205-8c2168555891', metadata={'source': 'utils/db/Caregiver Manual by Help Age International.pdf', 'page': 146}, page_content='health issues \\n• To discuss how medications may be affecting sleep \\n• To get support and guidance with challenges related to dementia \\n• Remember to maintain confidentiality in any discussions \\n \\nYou should also seek help if you are having problems sleeping or are \\nfeeling overwhelmed or exhausted. \\n \\nWhen to see advice from a health care professional'), Document(id='91ed3476-e8aa-4bfc-a10d-f3da7d2c9d89', metadata={'title': 'How Do You Assist Someone to Walk in Care', 'description': 'How to assist someone getting off the chair, walking and sitting back.', 'tags': ['walking', 'assisted walking', 'sitting', 'standing'], 'source': 'ProTrainings Europe', 'url': 'https://www.youtube.com/watch?v=3KHgy7zQ4MA', 'category': 'walking'}, page_content='How Do You Assist Someone to Walk in Care How to assist someone getting off the chair, walking and sitting back. walking assisted walking sitting standing'), Document(id='f61110db-aa3e-496f-8366-b4ec95e36734', metadata={'source': 'utils/db/Caregiver Manual by Help Age International.pdf', 'page': 69}, page_content='They need to be aware that some older people may lack balance, good vision and \\nhearing.  \\n \\nPromote safe driving \\nDiscuss these safety tips with the person you care for.  \\n• Check the route before leaving home. It is safer to drive only on familiar roads \\nthat are close to home.  \\n• Try to avoid right hand turns (if cars drive on the left), or left hand turns (if cars \\ndrive on the right).  \\n• Avoid driving at night, and when tired or stressed.  \\n• Always wear a seat belt and make sure passengers wear their seat belts.  \\n• Wear your glasses and/or hearing aid, if you use them.  \\n• Avoid distractions when driving, such as eating, listening to the radio, chatting \\nor talking on the phone.  \\n• Make sure that the windscreen and car windows are all clean and clear.   \\n \\nThe decision to stop driving \\nThere is no set age when it is recommended that someone stops driving because we \\nall age differently. To help the older person know if they should stop driving ask them'), Document(id='b7f1c5d7-02fd-45e2-bd58-63f12b697ff9', metadata={'source': 'utils/db/Caregiver Manual by Help Age International.pdf', 'page': 210}, page_content='Dressing and undressing │ Page 1/6 \\n  \\n \\n \\n \\n \\nDressing and undressing?  \\nFor many of us, how we dress and what we choose to wear is an important part of our \\nidentity and contributes to our self confidence and emotional wellbeing. As we get \\nolder, we might start to struggle to dress and undress ourselves, either in relation to \\nthe physical demands of dressing, or in terms of being able to decide what to wear. \\nGetting dressed and undressed are intimate and personal acts and having to ask for \\nsupport can be challenging. As a caregiver, it is important that you provide support in \\na way that ensures the privacy, modesty and dignity of the person you are supporting \\nis upheld and promoted. You will also need to be aware of the religious, cultural and \\nspiritual beliefs of the person you are supporting, which may influence how they \\ndress.  \\n \\nReasons dressing and undressing might become difficult \\nThere could be a number of reasons why the person you are supporting is no longer')]}\n",
      "Final Response:\n",
      "content=\"It’s understandable that this situation can feel overwhelming. Let’s find a solution together.\\n\\nTo turn your dad in bed safely, here’s the first step:\\n\\n1. **If possible, ask a second caregiver or family member to help.** They can stand on the other side of the bed.\\n\\nWould you like to continue with the next steps? If you're also looking for guidance on assisting someone to walk, I found a helpful video that demonstrates how to assist someone getting off a chair, walking, and sitting back down. You can check it out [here](https://www.youtube.com/watch?v=3KHgy7zQ4MA). Let me know how else I can assist you!\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 139, 'prompt_tokens': 157, 'total_tokens': 296, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0705bf87c0', 'finish_reason': 'stop', 'logprobs': None} id='run-71c3164c-cc02-42b3-9b29-08331eaee98d-0' usage_metadata={'input_tokens': 157, 'output_tokens': 139, 'total_tokens': 296, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    },
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for ChatResponse\nanswer\n  Input should be a valid string [type=string_type, input_value=AIMessage(content=\"It’s...o': 0, 'reasoning': 0}}), input_type=AIMessage]\n    For further information visit https://errors.pydantic.dev/2.9/v/string_type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m service\u001b[38;5;241m.\u001b[39mquery(request)\n",
      "Cell \u001b[0;32mIn[10], line 228\u001b[0m, in \u001b[0;36mChatService.query\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinal Response:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28mprint\u001b[39m(final_response)\n\u001b[0;32m--> 228\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mChatResponse\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchat_history\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat_history_store\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m    \u001b[49m\u001b[43msources\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_extract_cleaned_sources\u001b[49m\u001b[43m(\u001b[49m\u001b[43munified_context\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moriginal_context\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m    \u001b[49m\u001b[43mthumbnails\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fill_thumbnails\u001b[49m\u001b[43m(\u001b[49m\u001b[43munified_context\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvideos\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvideo_URLs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mvideo\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43murl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mvideo\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43munified_context\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvideos\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munified_context\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlocations\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfinal_response\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/batch22-pflegefreund/.venv2/lib/python3.12/site-packages/pydantic/main.py:212\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(self, **data)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[1;32m    211\u001b[0m __tracebackhide__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 212\u001b[0m validated_self \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_instance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m validated_self:\n\u001b[1;32m    214\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    215\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA custom validator is returning a value other than `self`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    216\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReturning anything other than `self` from a top level model validator isn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt supported when validating via `__init__`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    217\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSee the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    218\u001b[0m         category\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    219\u001b[0m     )\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for ChatResponse\nanswer\n  Input should be a valid string [type=string_type, input_value=AIMessage(content=\"It’s...o': 0, 'reasoning': 0}}), input_type=AIMessage]\n    For further information visit https://errors.pydantic.dev/2.9/v/string_type"
     ]
    }
   ],
   "source": [
    "response = await service.query(request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
